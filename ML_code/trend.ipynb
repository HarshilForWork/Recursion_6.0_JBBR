{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('lalal.json', 'r') as file:\n",
    "    uptownfunk_data = json.load(file)\n",
    "\n",
    "# Extract unique words from the JSON content\n",
    "keywords = list(set(word.lower() for entry in uptownfunk_data if 'text' in entry for word in entry['text'].split()))\n",
    "regions = [\n",
    "    \"IN\" \n",
    "]\n",
    "timeframe = \"now 7-d\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me', 'did', \"it's\", 'left', 'bankai', 'later', \"doesn't\", 'se', 'trapped', 'derivative', 'no', 'wow', 'you', 'knew', 'non-gmo', 'steel', 'bears', 'top', 'got', 'business', 'for', 'supporting', 'their', 'my', '811', 'name', 'c', 'feet', 'which', 'yeah', 'some', 'on', 'morning', 'syrup', 'pocahontas', 'already', 'tastes', 'rest', 'found', 'daily', 'bottom', 'support', 'oats', 'had', 'fructose', 'oil', 'foods', 'shopped', 'terrible', 'better', 'rinds', 'steal', 'brings', \"what's\", 'black', 'doing', 'thousand', 'freedom', 'met', 'of', 'go', 'want', 'elements', 'uh', 'baby', 'roasted', 'three', 'bottle', 'pay', 'in', 'ions', 'lavender', 'is', \"everything's\", 'what', 'are', 'earliest', 'only', 'fund', 'mouths', 'history', 'wonder', 'kombucha', 'dragonball', 'four', 'i', 'brother', 'puffs', 'sweet', 'upon', 'but', 'work', 'whole', 'mean', 'insane', 'stuff', 'looking', '70', 'dollars', 'a', 'soul', 'wished', 'oh', 'time', 'rash', 'moxie', '17', 'northie', 'paper', 'feed', 'by', 'from', 'into', 'was', '[applause]', 'comparison', ']', 'yucky', 'direct', 'babe', 'years', 'make', 'bezos', 'sure', 'david', 'brand', 'spit', \"you've\", 'shrooms', 'haven', 'get', 'listen', 'this', '-', '__â', \"i'm\", 'god', 'or', 'gummy', 'vile', 'box', 'extra', 'hearts', 'look', 'very', 'child', 'the', 'give', 'covered', '[music]', 'clone', 'high-calorie', 'thing', 'beans', 'form', 'truffles', 'human', 'parents', 'past', 'season', 'try', 'anniversary', 'jeff', 'headache', 'trust', 'little', 'trying', 'worth', 'glenn', 'code', 'write', 'snacks', '$80', \"can't\", 'story', 'these', 'hundred', 'pork', 'heart', 'society', 'megatron', 'holiday', 'mastered', 'soon', 'going', 'egg', 'fruit', 'your', 'different', 'raw', 'however', 'maple', 'many', 'jutsu', 'idea', 'check', 'sixth', 'grapefruits', 'every', \"don't\", 'times', 'industry', 'market', 'wealthy', 'oils', 'legal', 'find', 'old', \"that's\", 'class', '[â', 'have', 'there', 'shut', 'video', 'smash', 'not', 'accept', 'pasta', 'more', 'her', 'as', 'been', 'another', 'boss', 'granted', 'yell', '19', 'yummy', 'carry', \"you're\", 'school', 'salt', 'dare', 'excuse', 'none', 'store', 'calling', 'chicken', 'be', 'sea', 'each', 'think', 'farmers', '$12', 'toilet', 'year', 'never', 'it', 'like', 'at', 'samples', 'lawyer', '$4', 'wings', 'ah', '246', 'good', 'all', 'free-range', 'free', 'do', 'sausage', 'piece', 'daddy', 'smell', 'before', 'playing', 'yes', 'our', 'broth', 'truffle', 'ago', 'she', 'those', 'take', 'believe', 'made', 'cupcakes', 'natural', 'if', 'wish', 'will', 'yep', 'peach', 'big', 'excellent', 'could', 'then', 'reducing', 'local', 'subscribe', 'can', 'worker', 'cannot', 'ever', \"let's\", 'gold', 'cheetos', 'section', 'up', 'one', 'two', 'flavor', 'know', \"there's\", 'great', 'now', 'he', 'ecstasy', 'insist', 'organic', 'produce', 'course', 'disgusting', 'resonate', 'practicing', 'aisle', 'okay', 'well', 'quote', 'click', 'specially', 'would', 'shadow', 'real', 'bit', 'come', 'graduated', 'we', 'that', 'down', 'man', 'micros', 'rich', \"other's\", 'line', 'to', 'here', 'half', 'plains', 'really', 'just', 'thirty', 'corn', 'seeds', 'see', 'wind', 'eat', 'sugary', 'doctor', 'with', 'guess', 'basic', 'so', 'and', 'tasty', 'cheese', 'salty', 'almonds', 'out', 'wait', 'earth', 'freebies', 'bean', 'mother', 'essential', 'how', 'high', 'right', 'diet', 'than', 'mommy', 'always', 'cents', 'healthy', '120', 'sir', 'color', 'harvested'}\n"
     ]
    }
   ],
   "source": [
    "# Extract unique words from the JSON content, convert to lowercase, and create a set\n",
    "print(set(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_keywords(keywords, chunk_size):\n",
    "    \"\"\"\n",
    "    Splits a list of keywords into chunks of a specified size.\n",
    "    \n",
    "    Args:\n",
    "        keywords (list): List of keywords to split.\n",
    "        chunk_size (int): Size of each chunk.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of keyword chunks.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(keywords), chunk_size):\n",
    "        yield keywords[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_search_trends(keywords, regions, timeframe):\n",
    "    \"\"\"\n",
    "    Analyzes search trends for given keywords and regions with delays to avoid rate limits.\n",
    "    \n",
    "    Args:\n",
    "        keywords (list): List of keywords to analyze (up to 5 per request).\n",
    "        regions (list): List of region codes (e.g., 'US', 'UK').\n",
    "        timeframe (str): Timeframe for trends (e.g., 'today 5-y').\n",
    "    \n",
    "    Returns:\n",
    "        dict: Trend data for each region.\n",
    "    \"\"\"\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)  # Initialize pytrends\n",
    "    all_data = {}\n",
    "\n",
    "    # Split keywords into chunks of 5\n",
    "    keyword_chunks = chunk_keywords(keywords, chunk_size=5)\n",
    "\n",
    "    for region in regions:\n",
    "        for chunk in keyword_chunks:\n",
    "            try:\n",
    "                # Build payload and fetch data\n",
    "                pytrends.build_payload(chunk, cat=0, timeframe=timeframe, geo=region)\n",
    "                data = pytrends.interest_over_time()\n",
    "                if not data.empty:\n",
    "                    if region not in all_data:\n",
    "                        all_data[region] = data\n",
    "                    else:\n",
    "                        all_data[region] = all_data[region].join(data, how='outer')\n",
    "                \n",
    "                # Pause for 5 seconds to respect rate limits\n",
    "                time.sleep(5)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {region} with keywords {chunk}: {e}\")\n",
    "                # Wait longer if an error occurs, then continue\n",
    "                time.sleep(10)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PF\\Projects\\.venv\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n",
      "c:\\PF\\Projects\\.venv\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for IN with keywords ['later', \"doesn't\", 'se', 'trapped', 'derivative']: columns overlap but no suffix specified: Index(['isPartial'], dtype='object')\n",
      "Error fetching data for IN with keywords ['no', 'wow', 'you', 'knew', 'non-gmo']: The request failed: Google returned a response with code 429\n",
      "Error fetching data for IN with keywords ['steel', 'bears', 'top', 'got', 'business']: The request failed: Google returned a response with code 429\n",
      "Error fetching data for IN with keywords ['for', 'supporting', 'their', 'my', '811']: The request failed: Google returned a response with code 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PF\\Projects\\.venv\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for IN with keywords ['name', 'c', 'feet', 'which', 'yeah']: columns overlap but no suffix specified: Index(['isPartial'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PF\\Projects\\.venv\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for IN with keywords ['some', 'on', 'morning', 'syrup', 'pocahontas']: columns overlap but no suffix specified: Index(['isPartial'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PF\\Projects\\.venv\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for IN with keywords ['already', 'tastes', 'rest', 'found', 'daily']: columns overlap but no suffix specified: Index(['isPartial'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PF\\Projects\\.venv\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for IN with keywords ['bottom', 'support', 'oats', 'had', 'fructose']: columns overlap but no suffix specified: Index(['isPartial'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m, in \u001b[0;36manalyze_search_trends\u001b[1;34m(keywords, regions, timeframe)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         all_data[region] \u001b[38;5;241m=\u001b[39m \u001b[43mall_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Pause for 5 seconds to respect rate limits\u001b[39;00m\n",
      "File \u001b[1;32mc:\\PF\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10757\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[0;32m  10748\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10749\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10750\u001b[0m             other,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10755\u001b[0m             validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10756\u001b[0m         )\n\u001b[1;32m> 10757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m  10763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m  10764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10765\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\PF\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    170\u001b[0m op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m     left_df,\n\u001b[0;32m    172\u001b[0m     right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m     validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m )\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\PF\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\PF\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\PF\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2721\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lsuffix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rsuffix:\n\u001b[1;32m-> 2721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns overlap but no suffix specified: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_rename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrenamer\u001b[39m(x, suffix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['isPartial'], dtype='object')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Execute analysis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trend_data \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_search_trends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region, data \u001b[38;5;129;01min\u001b[39;00m trend_data\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[28], line 37\u001b[0m, in \u001b[0;36manalyze_search_trends\u001b[1;34m(keywords, regions, timeframe)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with keywords \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;66;03m# Wait longer if an error occurs, then continue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m             \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute analysis\n",
    "trend_data = analyze_search_trends(keywords, regions, timeframe)\n",
    "\n",
    "# Display results\n",
    "for region, data in trend_data.items():\n",
    "    print(f\"Trends for {region}:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "\n",
    "pytrends = TrendReq(hl='en-US', tz=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Blockchain\n",
      "geoName                   \n",
      "Afghanistan              9\n",
      "Albania                 17\n",
      "Algeria                  6\n",
      "American Samoa           0\n",
      "Andorra                 31\n"
     ]
    }
   ],
   "source": [
    "# Fetch interest by region data\n",
    "region_data = pytrends.interest_by_region(resolution='US', inc_low_vol=True, inc_geo_code=False)\n",
    "print(region_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list = [\"Blockchain\"]\n",
    "pytrends.build_payload(kw_list, cat=0, timeframe='today 5-y', geo='', gprop='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
